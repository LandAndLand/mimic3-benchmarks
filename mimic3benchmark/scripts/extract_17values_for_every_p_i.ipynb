{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('normal')",
   "metadata": {
    "interpreter": {
     "hash": "f00aa4d08b5dbe9a4f4240f8270de016ce9f4f5bdff9dc19e01b094329a335d5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "source": [
    "## 声明：itemid_to_variable_map.csv既包含mimic iii 的d_items表的内容， 又包含d_labitems表的内容。\n",
    "比如itemid_to_variable_map.csv LEVEL2为\"Albumin\"的第三条数据的itemid是50862，来自于d_labitems表；\n",
    "\n",
    "第四条数据的itemid是772，来自于d_items表。\n",
    "\n",
    "## itemid_to_variable_map.csv的MIMIC LABEL列是两个表的LABEL的值\n",
    "## d_labitems表的组成是：\n",
    "\n",
    "ROW_ID\tITEMID\tLABEL\tFLUID\tCATEGORY\tLOINC_CODE\t\n",
    "\n",
    "\n",
    "## d_items表的组成是：\n",
    "ROW_ID\tITEMID\tLABEL\tABBREVIATION\tDBSOURCE\tLINKSTO\tCATEGORY\tUNITNAME\tPARAM_TYPE\tCONCEPTID\t\n",
    "\n",
    "其中，LINKSTO值为chartevents\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_root_path = 'data/root'\n",
    "variable_map_file = '../resources/itemid_to_variable_map.csv'  # CSV containing ITEMID-to-VARIABLE map.\n",
    "reference_range_file = '../resources/variable_ranges.csv'      # CSV containing reference ranges for VARIABLEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    LEVEL2 LEVEL1  OUTLIER LOW  VALID LOW  IMPUTE  VALID HIGH  OUTLIER HIGH\n27  Height    NaN          0.0        0.0   170.0       240.0         275.0\nIndex(['LEVEL2', 'LEVEL1', 'ALTERNATIVE', 'STATUS', 'STATUS NOTE', 'ITEMID',\n       'MIMIC LABEL', 'UNITNAME', 'LINKSTO', 'COUNT', 'CATEGORY', 'CONCEPTID',\n       'FLUID', 'LOINC_CODE', 'DBSOURCE', 'Unnamed: 15', 'PARAM_TYPE', 'NOTE'],\n      dtype='object')\n0        219475\n1         41594\n2         37625\n3        146697\n4         31022\n          ...  \n13228         0\n13229         0\n13230         0\n13231         0\n13232         0\nName: COUNT, Length: 13233, dtype: int64\n                         LEVEL2                    LEVEL1  \\\n0      Alanine aminotransferase  Alanine aminotransferase   \n1      Alanine aminotransferase  Alanine aminotransferase   \n2      Alanine aminotransferase  Alanine aminotransferase   \n3                       Albumin                   Albumin   \n4                       Albumin                   Albumin   \n...                         ...                       ...   \n13228                       NaN                       NaN   \n13229                       NaN                       NaN   \n13230                       NaN                       NaN   \n13231                       NaN                       NaN   \n13232                       NaN                       NaN   \n\n                                     ALTERNATIVE  STATUS STATUS NOTE  ITEMID  \\\n0                                            ALT  verify         NaN   50861   \n1                                            ALT  verify         NaN     769   \n2                                            ALT  verify         NaN  220644   \n3                                            NaN  verify         NaN   50862   \n4                                            NaN  verify         NaN     772   \n...                                          ...     ...         ...     ...   \n13228  Barrier precautions in place (Intubation)     NaN         NaN  228643   \n13229          Barrier precautions in place (LP)     NaN         NaN  228644   \n13230       Barrier precautions in place (PACEN)     NaN         NaN  228645   \n13231       Barrier precautions in place (THCEN)     NaN         NaN  228646   \n13232      Barrier precautions in place (A-Line)     NaN         NaN  228647   \n\n                                     MIMIC LABEL UNITNAME      LINKSTO  \\\n0                 ALANINE AMINOTRANSFERASE (ALT)      NaN    labevents   \n1                                            ALT      NaN  chartevents   \n2                                            ALT     None  chartevents   \n3                                        ALBUMIN      NaN    labevents   \n4                                 Albumin (>3.2)      NaN  chartevents   \n...                                          ...      ...          ...   \n13228  Barrier precautions in place (Intubation)      NaN          NaN   \n13229          Barrier precautions in place (LP)      NaN          NaN   \n13230       Barrier precautions in place (PACEN)      NaN          NaN   \n13231       Barrier precautions in place (THCEN)      NaN          NaN   \n13232      Barrier precautions in place (A-Line)      NaN          NaN   \n\n        COUNT                 CATEGORY  CONCEPTID  FLUID LOINC_CODE  \\\n0      219475                CHEMISTRY        NaN  BLOOD   6/1/1742   \n1       41594                  Enzymes        NaN    NaN        NaN   \n2       37625                     Labs        NaN    NaN        NaN   \n3      146697                CHEMISTRY        NaN  BLOOD   7/1/1751   \n4       31022                Chemistry        NaN    NaN        NaN   \n...       ...                      ...        ...    ...        ...   \n13228       0               Intubation        NaN    NaN        NaN   \n13229       0          Lumbar Puncture        NaN    NaN        NaN   \n13230       0             Paracentesis        NaN    NaN        NaN   \n13231       0            Thoracentesis        NaN    NaN        NaN   \n13232       0  Arterial Line Insertion        NaN    NaN        NaN   \n\n         DBSOURCE  Unnamed: 15 PARAM_TYPE  NOTE  \n0             NaN          NaN        NaN   NaN  \n1         carevue          NaN        NaN   NaN  \n2      metavision          NaN    Numeric   NaN  \n3             NaN          NaN        NaN   NaN  \n4         carevue          NaN        NaN   NaN  \n...           ...          ...        ...   ...  \n13228  metavision          NaN       Text   NaN  \n13229  metavision          NaN       Text   NaN  \n13230  metavision          NaN       Text   NaN  \n13231  metavision          NaN       Text   NaN  \n13232  metavision          NaN       Text   NaN  \n\n[13233 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "variable_range = pd.read_csv(reference_range_file)\n",
    "print(variable_range[variable_range['LEVEL2'] =='Height' ])\n",
    "\n",
    "itemid_map_variable = pd.read_csv(variable_map_file)\n",
    "print(itemid_map_variable.columns)\n",
    "print(itemid_map_variable.COUNT)\n",
    "print(itemid_map_variable)"
   ]
  },
  {
   "source": [
    " ## 去掉itemid_to_variable_map.csv里面LEVEL2为空、或者COUNT<=0的数据，这样就得到了17个变量的name和itemid\n",
    "### paper 里面说：the first column （LEVEL2）is the name of the variable in our benchmark\n",
    " ### COUNT 是什么？LEVEL1又是什么？"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_itemid_to_variable_map(fn, variable_column='LEVEL2'):\n",
    "    var_map = pd.read_csv(fn, index_col=None).fillna('').astype(str)\n",
    "    # var_map[variable_column] = var_map[variable_column].apply(lambda s: s.lower())\n",
    "    LAEVL_2_NULL_NUM_MASK = var_map[variable_column] == ''\n",
    "    #print(f'var_map中 LEVEL2 值为null的数据有 {len(var_map[LAEVL_2_NULL_NUM_MASK])}个 ')\n",
    "    var_map.COUNT = var_map.COUNT.astype(int)\n",
    "    # ???COUNT是什么意思\n",
    "    var_map = var_map[(var_map[variable_column] != '') & (var_map.COUNT > 0)]\n",
    "    # ready代表什么？怎么来的？\n",
    "    var_map = var_map[(var_map.STATUS == 'ready')]\n",
    "    var_map.ITEMID = var_map.ITEMID.astype(int)\n",
    "    var_map = var_map[[variable_column, 'ITEMID', 'MIMIC LABEL']].set_index('ITEMID')\n",
    "    return var_map.rename({variable_column: 'VARIABLE', 'MIMIC LABEL': 'MIMIC_LABEL'}, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                        VARIABLE                        MIMIC_LABEL\n",
       "ITEMID                                                             \n",
       "3348       Capillary refill rate                   Capillary Refill\n",
       "115        Capillary refill rate           Capillary Refill [Right]\n",
       "8377       Capillary refill rate            Capillary Refill [Left]\n",
       "8368    Diastolic blood pressure            Arterial BP [Diastolic]\n",
       "220051  Diastolic blood pressure  Arterial Blood Pressure diastolic\n",
       "...                          ...                                ...\n",
       "3580                      Weight               Present Weight  (kg)\n",
       "3693                      Weight                          Weight Kg\n",
       "3581                      Weight               Present Weight  (lb)\n",
       "226531                    Weight            Admission Weight (lbs.)\n",
       "3582                      Weight               Present Weight  (oz)\n",
       "\n",
       "[114 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VARIABLE</th>\n      <th>MIMIC_LABEL</th>\n    </tr>\n    <tr>\n      <th>ITEMID</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3348</th>\n      <td>Capillary refill rate</td>\n      <td>Capillary Refill</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>Capillary refill rate</td>\n      <td>Capillary Refill [Right]</td>\n    </tr>\n    <tr>\n      <th>8377</th>\n      <td>Capillary refill rate</td>\n      <td>Capillary Refill [Left]</td>\n    </tr>\n    <tr>\n      <th>8368</th>\n      <td>Diastolic blood pressure</td>\n      <td>Arterial BP [Diastolic]</td>\n    </tr>\n    <tr>\n      <th>220051</th>\n      <td>Diastolic blood pressure</td>\n      <td>Arterial Blood Pressure diastolic</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3580</th>\n      <td>Weight</td>\n      <td>Present Weight  (kg)</td>\n    </tr>\n    <tr>\n      <th>3693</th>\n      <td>Weight</td>\n      <td>Weight Kg</td>\n    </tr>\n    <tr>\n      <th>3581</th>\n      <td>Weight</td>\n      <td>Present Weight  (lb)</td>\n    </tr>\n    <tr>\n      <th>226531</th>\n      <td>Weight</td>\n      <td>Admission Weight (lbs.)</td>\n    </tr>\n    <tr>\n      <th>3582</th>\n      <td>Weight</td>\n      <td>Present Weight  (oz)</td>\n    </tr>\n  </tbody>\n</table>\n<p>114 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "source": [
    "itemid_map_variable  = read_itemid_to_variable_map(variable_map_file, variable_column='LEVEL2')\n",
    "itemid_map_variable"
   ]
  },
  {
   "source": [
    "## 得到17个所使用的临床变量"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Capillary refill rate', 'Diastolic blood pressure',\n",
       "       'Fraction inspired oxygen', 'Glascow coma scale eye opening',\n",
       "       'Glascow coma scale motor response', 'Glascow coma scale total',\n",
       "       'Glascow coma scale verbal response', 'Glucose', 'Heart Rate',\n",
       "       'Height', 'Mean blood pressure', 'Oxygen saturation', 'pH',\n",
       "       'Respiratory rate', 'Systolic blood pressure', 'Temperature',\n",
       "       'Weight'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "source": [
    "# 得到17个所使用的临床变量\n",
    "variables = itemid_map_variable.VARIABLE.unique()\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "33798"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    "#os.listdir(subjects_root_path)\n",
    "len(os.listdir(subjects_root_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stays(subject_path):\n",
    "    stays = pd.read_csv(os.path.join(subject_path, 'stays.csv'), index_col=None)\n",
    "    stays.INTIME = pd.to_datetime(stays.INTIME)\n",
    "    stays.OUTTIME = pd.to_datetime(stays.OUTTIME)\n",
    "    stays.DOB = pd.to_datetime(stays.DOB)\n",
    "    stays.DOD = pd.to_datetime(stays.DOD)\n",
    "    stays.DEATHTIME = pd.to_datetime(stays.DEATHTIME)\n",
    "    stays.sort_values(by=['INTIME', 'OUTTIME'], inplace=True)\n",
    "    return stays\n",
    "\n",
    "\n",
    "def read_diagnoses(subject_path):\n",
    "    return pd.read_csv(os.path.join(subject_path, 'diagnoses.csv'), index_col=None)\n",
    "\n",
    "\n",
    "def read_events(subject_path, remove_null=True):\n",
    "    events = pd.read_csv(os.path.join(subject_path, 'events.csv'), index_col=None)\n",
    "    if remove_null:\n",
    "        events = events[events.VALUE.notnull()]\n",
    "    events.CHARTTIME = pd.to_datetime(events.CHARTTIME)\n",
    "    events.HADM_ID = events.HADM_ID.fillna(value=-1).astype(int)\n",
    "    events.ICUSTAY_ID = events.ICUSTAY_ID.fillna(value=-1).astype(int)\n",
    "    events.VALUEUOM = events.VALUEUOM.fillna('').astype(str)\n",
    "    # events.sort_values(by=['CHARTTIME', 'ITEMID', 'ICUSTAY_ID'], inplace=True)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_labels = ['4019', '4280', '41401', '42731', '25000', '5849', '2724', '51881', '53081', '5990', '2720',\n",
    "                    '2859', '2449', '486', '2762', '2851', '496', 'V5861', '99592', '311', '0389', '5859', '5070',\n",
    "                    '40390', '3051', '412', 'V4581', '2761', '41071', '2875', '4240', 'V1582', 'V4582', 'V5867',\n",
    "                    '4241', '40391', '78552', '5119', '42789', '32723', '49390', '9971', '2767', '2760', '2749',\n",
    "                    '4168', '5180', '45829', '4589', '73300', '5845', '78039', '5856', '4271', '4254', '4111',\n",
    "                    'V1251', '30000', '3572', '60000', '27800', '41400', '2768', '4439', '27651', 'V4501', '27652',\n",
    "                    '99811', '431', '28521', '2930', '7907', 'E8798', '5789', '79902', 'V4986', 'V103', '42832',\n",
    "                    'E8788', '00845', '5715', '99591', '07054', '42833', '4275', '49121', 'V1046', '2948', '70703',\n",
    "                    '2809', '5712', '27801', '42732', '99812', '4139', '3004', '2639', '42822', '25060', 'V1254',\n",
    "                    '42823', '28529', 'E8782', '30500', '78791', '78551', 'E8889', '78820', '34590', '2800', '99859',\n",
    "                    'V667', 'E8497', '79092', '5723', '3485', '5601', '25040', '570', '71590', '2869', '2763', '5770',\n",
    "                    'V5865', '99662', '28860', '36201', '56210']\n",
    "\n",
    "def extract_diagnosis_labels(diagnoses):\n",
    "    global diagnosis_labels\n",
    "    diagnoses['VALUE'] = 1\n",
    "    labels = diagnoses[['ICUSTAY_ID', 'ICD9_CODE', 'VALUE']].drop_duplicates()\\\n",
    "                      .pivot(index='ICUSTAY_ID', columns='ICD9_CODE', values='VALUE').fillna(0).astype(int)\n",
    "    for l in diagnosis_labels:\n",
    "        if l not in labels:\n",
    "            labels[l] = 0\n",
    "    labels = labels[diagnosis_labels]\n",
    "    return labels.rename(dict(zip(diagnosis_labels, ['Diagnosis ' + d for d in diagnosis_labels])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_episodic_data(stays, diagnoses):\n",
    "    data = {'Icustay': stays.ICUSTAY_ID, 'Age': stays.age, 'Length of Stay': stays.LOS,\n",
    "            'Mortality': stays.MORTALITY}\n",
    "    data.update(transform_gender(stays.GENDER))\n",
    "    data.update(transform_ethnicity(stays.ETHNICITY))\n",
    "    data['Height'] = np.nan\n",
    "    data['Weight'] = np.nan\n",
    "    data = pd.DataFrame(data).set_index('Icustay')\n",
    "    data = data[['Ethnicity', 'Gender', 'Age', 'Height', 'Weight', 'Length of Stay', 'Mortality']]\n",
    "    return data.merge(extract_diagnosis_labels(diagnoses), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_map = {'F': 1, 'M': 2, 'OTHER': 3, '': 0}\n",
    "def transform_gender(gender_series):\n",
    "    return {'Gender': gender_series.fillna('').apply(lambda s: g_map[s] if s in g_map else g_map['OTHER'])}\n",
    "e_map = {'ASIAN': 1,\n",
    "         'BLACK': 2,\n",
    "         'CARIBBEAN ISLAND': 2,\n",
    "         'HISPANIC': 3,\n",
    "         'SOUTH AMERICAN': 3,\n",
    "         'WHITE': 4,\n",
    "         'MIDDLE EASTERN': 4,\n",
    "         'PORTUGUESE': 4,\n",
    "         'AMERICAN INDIAN': 0,\n",
    "         'NATIVE HAWAIIAN': 0,\n",
    "         'UNABLE TO OBTAIN': 0,\n",
    "         'PATIENT DECLINED TO ANSWER': 0,\n",
    "         'UNKNOWN': 0,\n",
    "         'OTHER': 0,\n",
    "         '': 0}\n",
    "def transform_ethnicity(ethnicity_series):\n",
    "\n",
    "    def aggregate_ethnicity(ethnicity_str):\n",
    "        return ethnicity_str.replace(' OR ', '/').split(' - ')[0].split('/')[0]\n",
    "\n",
    "    ethnicity_series = ethnicity_series.apply(aggregate_ethnicity)\n",
    "    return {'Ethnicity': ethnicity_series.fillna('').apply(lambda s: e_map[s] if s in e_map else e_map['OTHER'])}\n"
   ]
  },
  {
   "source": [
    "### 只保留events.csv的指定的17个变量的记录，过滤掉其他的events.csv的记录"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\" \\na = events.iloc[:10]\\nprint('original a:')\\nprint(a)\\nb = itemid_map_variable.iloc[:5]\\nprint('b:')\\nprint(b)\\na = a.merge(b, left_on='ITEMID', right_index=True)\\nprint('after merged :')\\nprint(a) \\n\""
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "# 过滤示例：\n",
    "\"\"\" \n",
    "a = events.iloc[:10]\n",
    "print('original a:')\n",
    "print(a)\n",
    "b = itemid_map_variable.iloc[:5]\n",
    "print('b:')\n",
    "print(b)\n",
    "a = a.merge(b, left_on='ITEMID', right_index=True)\n",
    "print('after merged :')\n",
    "print(a) \n",
    "\"\"\""
   ]
  },
  {
   "source": [
    "## 需要对events.csv的一些离散数据的value进行处理，比如一些字符串转换为0/1（normal/abnormal）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRR: strings with brisk, <3 normal, delayed, or >3 abnormal\n",
    "'''\n",
    "毛细血管充盈率： <3为正常, 设置为1\n",
    "'''\n",
    "def clean_crr(df):\n",
    "    v = Series(np.zeros(df.shape[0]), index=df.index)\n",
    "    v[:] = np.nan\n",
    "\n",
    "    # when df.VALUE is empty, dtype can be float and comparision with string\n",
    "    # raises an exception, to fix this we change dtype to str\n",
    "    df_value_str = df.VALUE.astype(str)\n",
    "\n",
    "    v.loc[(df_value_str == 'Normal <3 secs') | (df_value_str == 'Brisk')] = 0\n",
    "    v.loc[(df_value_str == 'Abnormal >3 secs') | (df_value_str == 'Delayed')] = 1\n",
    "    return v\n",
    "\n",
    "'''\n",
    "舒张压：\n",
    "成人正常的舒张压为60~90mmHg(12kpa) ，血压的单位为千帕，1千帕=7.6mmHg\n",
    "\n",
    "不知道这里为什么要用到正则匹配，舒张压在mimic里面的难道不是一个数值吗？\n",
    "回答这个问题：\n",
    "舒张压分为两部分： systolic（收缩） 和 diastolic（舒张）；mimic是分开存储的，value是数字，\n",
    "然后，正则匹配接收的类型是：“ 数字 / 数字”\n",
    "\n",
    "systolic blood pressure，SBP，收缩压；\n",
    "diastolic blood pressure，DBP，舒张压。\n",
    "'''   \n",
    "# SBP: some are strings of type SBP/DBP\n",
    "def clean_sbp(df):\n",
    "    v = df.VALUE.astype(str).copy()\n",
    "    # 找出存储格式是 SBP/DBP的字符串，分割出SBP\n",
    "    idx = v.apply(lambda s: '/' in s)\n",
    "    v.loc[idx] = v[idx].apply(lambda s: re.match('^(\\d+)/(\\d+)$', s).group(1))\n",
    "    return v.astype(float)\n",
    "\n",
    "def clean_dbp(df):\n",
    "    v = df.VALUE.astype(str).copy()\n",
    "    idx = v.apply(lambda s: '/' in s)\n",
    "    # # 找出存储格式是 SBP/DBP的字符串，分割出DBP\n",
    "    v.loc[idx] = v[idx].apply(lambda s: re.match('^(\\d+)/(\\d+)$', s).group(2))\n",
    "    return v.astype(float)\n",
    "\n",
    "\n",
    "# FIO2: many 0s, some 0<x<0.2 or 1<x<20\n",
    "def clean_fio2(df):\n",
    "    v = df.VALUE.astype(float).copy()\n",
    "\n",
    "    ''' The line below is the correct way of doing the cleaning, since we will not compare 'str' to 'float'.\n",
    "    If we use that line it will create mismatches from the data of the paper in ~50 ICU stays.\n",
    "    The next releases of the benchmark should use this line.\n",
    "    '''\n",
    "    # idx = df.VALUEUOM.fillna('').apply(lambda s: 'torr' not in s.lower()) & (v>1.0)\n",
    "\n",
    "    ''' The line below was used to create the benchmark dataset that the paper used. Note this line will not work\n",
    "    in python 3, since it may try to compare 'str' to 'float'.\n",
    "    '''\n",
    "    # idx = df.VALUEUOM.fillna('').apply(lambda s: 'torr' not in s.lower()) & (df.VALUE > 1.0)\n",
    "\n",
    "    ''' The two following lines implement the code that was used to create the benchmark dataset that the paper used.\n",
    "    This works with both python 2 and python 3.\n",
    "    '''\n",
    "    is_str = np.array(map(lambda x: type(x) == str, list(df.VALUE)), dtype=np.bool)\n",
    "    idx = df.VALUEUOM.fillna('').apply(lambda s: 'torr' not in s.lower()) & (is_str | (~is_str & (v > 1.0)))\n",
    "\n",
    "    v.loc[idx] = v[idx] / 100.\n",
    "    return v\n",
    "\n",
    "\n",
    "# GLUCOSE, PH: sometimes have ERROR as value\n",
    "def clean_lab(df):\n",
    "    v = df.VALUE.copy()\n",
    "    idx = v.apply(lambda s: type(s) is str and not re.match('^(\\d+(\\.\\d*)?|\\.\\d+)$', s))\n",
    "    v.loc[idx] = np.nan\n",
    "    return v.astype(float)\n",
    "\n",
    "\n",
    "# O2SAT: small number of 0<x<=1 that should be mapped to 0-100 scale\n",
    "def clean_o2sat(df):\n",
    "    # change \"ERROR\" to NaN\n",
    "    v = df.VALUE.copy()\n",
    "    idx = v.apply(lambda s: type(s) is str and not re.match('^(\\d+(\\.\\d*)?|\\.\\d+)$', s))\n",
    "    v.loc[idx] = np.nan\n",
    "\n",
    "    v = v.astype(float)\n",
    "    idx = (v <= 1)\n",
    "    v.loc[idx] = v[idx] * 100.\n",
    "    return v\n",
    "\n",
    "\n",
    "# Temperature: map Farenheit to Celsius, some ambiguous 50<x<80\n",
    "def clean_temperature(df):\n",
    "    v = df.VALUE.astype(float).copy()\n",
    "    idx = df.VALUEUOM.fillna('').apply(lambda s: 'F' in s.lower()) | df.MIMIC_LABEL.apply(lambda s: 'F' in s.lower()) | (v >= 79)\n",
    "    v.loc[idx] = (v[idx] - 32) * 5. / 9\n",
    "    return v\n",
    "\n",
    "\n",
    "# Weight: some really light/heavy adults: <50 lb, >450 lb, ambiguous oz/lb\n",
    "# Children are tough for height, weight\n",
    "def clean_weight(df):\n",
    "    v = df.VALUE.astype(float).copy()\n",
    "    # ounces\n",
    "    idx = df.VALUEUOM.fillna('').apply(lambda s: 'oz' in s.lower()) | df.MIMIC_LABEL.apply(lambda s: 'oz' in s.lower())\n",
    "    v.loc[idx] = v[idx] / 16.\n",
    "    # pounds\n",
    "    idx = idx | df.VALUEUOM.fillna('').apply(lambda s: 'lb' in s.lower()) | df.MIMIC_LABEL.apply(lambda s: 'lb' in s.lower())\n",
    "    v.loc[idx] = v[idx] * 0.453592\n",
    "    return v\n",
    "\n",
    "\n",
    "# Height: some really short/tall adults: <2 ft, >7 ft)\n",
    "# Children are tough for height, weight\n",
    "def clean_height(df):\n",
    "    v = df.VALUE.astype(float).copy()\n",
    "    idx = df.VALUEUOM.fillna('').apply(lambda s: 'in' in s.lower()) | df.MIMIC_LABEL.apply(lambda s: 'in' in s.lower())\n",
    "    v.loc[idx] = np.round(v[idx] * 2.54)\n",
    "    return v\n",
    "\n",
    "# ETCO2: haven't found yet\n",
    "# Urine output: ambiguous units (raw ccs, ccs/kg/hr, 24-hr, etc.)\n",
    "# Tidal volume: tried to substitute for ETCO2 but units are ambiguous\n",
    "# Glascow coma scale eye opening\n",
    "# Glascow coma scale motor response\n",
    "# Glascow coma scale total\n",
    "# Glascow coma scale verbal response\n",
    "# Heart Rate\n",
    "# Respiratory rate\n",
    "# Mean blood pressure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_fns = {\n",
    "    'Capillary refill rate': clean_crr,    # 毛细血管充盈率, 需要处理为0、1，分别代表normal 、 abnormal\n",
    "    'Diastolic blood pressure': clean_dbp, # 舒张压； 处理SBP/DBP格式的数据，分离出SBP\n",
    "    'Systolic blood pressure': clean_sbp,  # 收缩压； 处理SBP/DBP格式的数据，分离出DBP\n",
    "    'Fraction inspired oxygen': clean_fio2,\n",
    "    'Oxygen saturation': clean_o2sat,\n",
    "    'Glucose': clean_lab,\n",
    "    'pH': clean_lab,\n",
    "    'Temperature': clean_temperature,\n",
    "    'Weight': clean_weight,\n",
    "    'Height': clean_height\n",
    "}\n",
    "\n",
    "\n",
    "def clean_events(events):\n",
    "    for var_name, clean_fn in clean_fns.items():\n",
    "        idx = (events.VARIABLE == var_name)\n",
    "        try:\n",
    "            events.loc[idx, 'VALUE'] = clean_fn(events[idx])\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(\"Exception in clean_events:\", clean_fn.__name__, e)\n",
    "            print(traceback.format_exc())\n",
    "            print(\"number of rows:\", np.sum(idx))\n",
    "            print(\"values:\", events[idx])\n",
    "            exit()\n",
    "    return events.loc[events.VALUE.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_events_to_timeseries(events, variable_column='VARIABLE', variables=[]):\n",
    "    # 先把metadata按照时间递增的顺序排列\n",
    "    metadata = events[['CHARTTIME', 'ICUSTAY_ID']].sort_values(by=['CHARTTIME', 'ICUSTAY_ID'])\\\n",
    "                    .drop_duplicates(keep='first').set_index('CHARTTIME')\n",
    "    timeseries = events[['CHARTTIME', variable_column, 'VALUE']]\\\n",
    "                    .sort_values(by=['CHARTTIME', variable_column, 'VALUE'], axis=0)\\\n",
    "                    .drop_duplicates(subset=['CHARTTIME', variable_column], keep='last')\n",
    "    timeseries = timeseries.pivot(index='CHARTTIME', columns=variable_column, values='VALUE')\\\n",
    "                    .merge(metadata, left_index=True, right_index=True)\\\n",
    "                    .sort_index(axis=0).reset_index()\n",
    "    for v in variables:\n",
    "        if v not in timeseries:\n",
    "            timeseries[v] = np.nan\n",
    "    return timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events_for_stay(timeseries, icustayid, intime=None, outtime=None):\n",
    "    idx = (timeseries.ICUSTAY_ID == icustayid)\n",
    "    # 这种情况避免了ICUSTAY_ID为空的情况\n",
    "    if intime is not None and outtime is not None:\n",
    "        idx = idx | ((timeseries.CHARTTIME >= intime) & (timeseries.CHARTTIME <= outtime))\n",
    "    timeseries = timeseries[idx]\n",
    "    del timeseries['ICUSTAY_ID']\n",
    "    return timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_valid_from_timeseries(timeseries, variable):\n",
    "    if variable in timeseries:\n",
    "        idx = timeseries[variable].notnull()\n",
    "        # 判断'Height'是否有非空的取值\n",
    "        if idx.any():\n",
    "            loc = np.where(idx)[0][0]\n",
    "            return timeseries[variable].iloc[loc]\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hours_elpased_to_events(events, dt, remove_charttime=True):\n",
    "    events = events.copy()\n",
    "    # 计算测量某项的操作是入住ICU多少小时后进行的\n",
    "    events['HOURS'] = (events.CHARTTIME - dt).apply(lambda s: s / np.timedelta64(1, 's')) / 60./60\n",
    "    if remove_charttime:\n",
    "        del events['CHARTTIME']\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Iterating over subjects:   0%|          | 24/33798 [00:04<1:40:05,  5.62it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '431'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3573\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3574\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3575\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '431'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-1ea9b7f1c318>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m             'Error reading from disk for subject: {}\\n'.format(subject_id))\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# episodic_data是ICU信息和诊断信息的结合\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mepisodic_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massemble_episodic_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiagnoses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;31m# 只保留events.csv的指定的17个变量的记录，过滤掉其他的events.csv的记录\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     '''\n",
      "\u001b[1;32m<ipython-input-144-cd3585a8b34d>\u001b[0m in \u001b[0;36massemble_episodic_data\u001b[1;34m(stays, diagnoses)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Icustay'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Ethnicity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Gender'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Age'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Height'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Weight'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Length of Stay'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Mortality'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_diagnosis_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiagnoses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-143-0a6d544ecc0c>\u001b[0m in \u001b[0;36mextract_diagnosis_labels\u001b[1;34m(diagnoses)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdiagnosis_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdiagnosis_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiagnosis_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Diagnosis '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0md\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdiagnosis_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3042\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3043\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3044\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3046\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3119\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3120\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3121\u001b[1;33m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3123\u001b[0m         \u001b[1;31m# check if we are modifying a copy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3575\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3576\u001b[0m             \u001b[1;31m# This item wasn't present, just insert at end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3577\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3578\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m         \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mblkno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_fast_count_smallints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblknos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m             \u001b[0mblk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblkno\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\envs\\normal\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_fast_count_smallints\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m   2008\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_fast_count_smallints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m     \u001b[1;34m\"\"\"Faster version of set(arr) for sequences of small numbers.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m     \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m     \u001b[0mnz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbincount\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for subject_dir in tqdm(os.listdir(subjects_root_path), desc='Iterating over subjects'):\n",
    "    #subject_dir = str(3)\n",
    "    dn = os.path.join(subjects_root_path, subject_dir)\n",
    "    try:\n",
    "        subject_id = int(subject_dir)\n",
    "        if not os.path.isdir(dn):\n",
    "            raise Exception\n",
    "    except:\n",
    "        sys.stderr.write(' {} doesnot exists\\n'.format(subject_id))\n",
    "\n",
    "    try:\n",
    "        # reading tables of this subject\n",
    "        stays = read_stays(os.path.join(subjects_root_path, subject_dir))\n",
    "        #print(stays)\n",
    "        diagnoses = read_diagnoses(\n",
    "            os.path.join(subjects_root_path, subject_dir))\n",
    "        #print('diagnose:')\n",
    "        #print(diagnoses)\n",
    "        events = read_events(os.path.join(subjects_root_path, subject_dir))\n",
    "    except:\n",
    "        sys.stderr.write(\n",
    "            'Error reading from disk for subject: {}\\n'.format(subject_id))\n",
    "    # episodic_data是ICU信息和诊断信息的结合\n",
    "    episodic_data = assemble_episodic_data(stays, diagnoses)\n",
    "    # 只保留events.csv的指定的17个变量的记录，过滤掉其他的events.csv的记录\n",
    "    '''\n",
    "    itemid_map_variable的内容是17个变量的itemid、benchmark_name、mimic_label;\n",
    "    itemid_map_variable的index是ITEMID\n",
    "    '''\n",
    "    events = events.merge(itemid_map_variable,\n",
    "                          left_on='ITEMID', right_index=True)\n",
    "    # 处理一下17个变量里面的部分变量的取值（清洗数据的过程）\n",
    "    events = clean_events(events)\n",
    "    \n",
    "    # 把events的内容按照时间递增的顺序排序，每一行包含17个列（17个变量）\n",
    "    # variables是17个临床变量所使用的的名字\n",
    "    timeseries = convert_events_to_timeseries(\n",
    "        events, variable_column='VARIABLE', variables=variables)\n",
    "    # print(timeseries)\n",
    "\n",
    "    # extracting separate episodes\n",
    "    # extracting separate episodes\n",
    "    # 遍历患者的每一次住院记录（很多病人只有1次住院记录）\n",
    "    for i in range(stays.shape[0]):\n",
    "        stay_id = stays.ICUSTAY_ID.iloc[i]\n",
    "        intime = stays.INTIME.iloc[i]\n",
    "        outtime = stays.OUTTIME.iloc[i]\n",
    "        # 得到某病人某次入住ICU的17个变量的测量记录\n",
    "        # 即episode是某个病人某次入住ICU的17个变量的所有测量记录\n",
    "        episode = get_events_for_stay(timeseries, stay_id, intime, outtime)\n",
    "        if episode.shape[0] == 0:\n",
    "            # no data for this episode\n",
    "            continue\n",
    "        # 计算测量某项的操作是入住ICU多少小时后进行的，并将递增的hour作为index\n",
    "        episode = add_hours_elpased_to_events(\n",
    "            episode, intime).set_index('HOURS').sort_index(axis=0)\n",
    "        if stay_id in episodic_data.index:\n",
    "            episodic_data.loc[stay_id, 'Weight'] = get_first_valid_from_timeseries(\n",
    "                episode, 'Weight')\n",
    "            episodic_data.loc[stay_id, 'Height'] = get_first_valid_from_timeseries(\n",
    "                episode, 'Height')\n",
    "        # episodic_data是ICU信息和诊断信息的结合\n",
    "        episodic_data.loc[episodic_data.index == stay_id].to_csv(os.path.join(subjects_root_path, subject_dir,\n",
    "                                                                              'episode{}.csv'.format(i+1)),\n",
    "                                                                 index_label='Icustay')\n",
    "        columns = list(episode.columns)\n",
    "        columns_sorted = sorted(columns, key=(\n",
    "            lambda x: \"\" if x == \"Hours\" else x))\n",
    "        #print(episode)\n",
    "        episode = episode[columns_sorted]\n",
    "        episode.to_csv(os.path.join(subjects_root_path, subject_dir, 'episode{}_timeseries.csv'.format(i+1)),\n",
    "                       index_label='Hours')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}