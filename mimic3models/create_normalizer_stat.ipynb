{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('normal')",
   "metadata": {
    "interpreter": {
     "hash": "304dd9ef76ad2d6bf7237ef8f6fde70bef676e0aff923029a3f8742854b86f8f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Computes means and standard variations on the train data and uses those statistics to normalize both the train and test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "# 导入上级目录\n",
    "sys.path.append('..')\n",
    "from mimic3models.preprocessing import Discretizer, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir='../mimic3benchmark/scripts/data/in-hospital-mortality'\n",
    "train_listfile = os.path.join(dataset_dir, 'train_listfile.csv')\n",
    "test_listfile = os.path.join(dataset_dir, 'test_listfile.csv')\n",
    "result_dir = 'result/mortality' \n",
    "config_path = 'resources/discretizer_config.json'\n",
    "output_dir = 'result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 'train'\n",
    "timestep = 1.0\n",
    "task = 'ihm'\n",
    "impute_strategy = 'previous'\n",
    "start_time = 'zero'\n",
    "store_masks=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "在父类里面完成\"partition\"_listfile.csv读取操作\n",
    "'''\n",
    "class Reader(object):\n",
    "    def __init__(self, dataset_dir='scripts/data/in-hospital-mortality', partition='train', listfile=None):\n",
    "        self.dataset = dataset_dir\n",
    "        self.current_index = 0\n",
    "        self.partition = partition\n",
    "        if listfile is None:\n",
    "            listfile_path = os.path.join(dataset_dir, self.partition+'_listfile.csv')\n",
    "        else:\n",
    "            listfile_path = listfile\n",
    "        # 获取\"partition\"_listfile.csv的内容\n",
    "        # 形式： *.csv, y_true\n",
    "        self.data = pd.read_csv(listfile_path)\n",
    "        self.data['y_true'] = self.data['y_true'].astype('int')\n",
    "        self.header = self.data.columns.values.tolist()\n",
    "    def get_number_of_examples(self):\n",
    "        return self.data.shape[0]\n",
    "    def random_shuffle(self, seed=None):\n",
    "        # 如果是训练集，打乱数据\n",
    "        if self.partition == 'train':\n",
    "            self.data = self.data.sample(frac=1.0)\n",
    "    #  reads the sample with the given index\n",
    "    def read_sample(self, index):\n",
    "        raise NotImplementedError()\n",
    "    #  read_next reads the next sample by using a cyclic counter inside\n",
    "    def read_next(self):\n",
    "        to_read_index = self.current_index\n",
    "        self.current_index += 1\n",
    "        if self.current_index == self.get_number_of_examples():\n",
    "            self.current_index = 0\n",
    "        return self.read_sample(to_read_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Reader for in-hospital moratality prediction task.\n",
    "\n",
    ":param dataset_dir:   Directory where timeseries files are stored.\n",
    ":param listfile:      Path to a listfile. If this parameter is left `None` then\n",
    "                        `dataset_dir/listfile.csv` will be used.\n",
    ":param period_length: Length of the period (in hours) from which the prediction is done.\n",
    "\"\"\"\n",
    "class InHospitalMortalityReader(Reader):\n",
    "    def __init__(self, dataset_dir='scripts/data/in-hospital-mortality', partition='train', period_length=48.0):\n",
    "        Reader.__init__(self, dataset_dir, listfile=None)\n",
    "        # self.data是父类读取的\"partition\"_listfile.csv的内容\n",
    "        self.data_x = self.data['patient_id_i'].values.tolist()\n",
    "        self.data_y = self.data['y_true'].values.tolist()\n",
    "        self.period_length = period_length\n",
    "        #print(self.data_x)\n",
    "    # 读取self.data_x的csv文件，提取里面的17个变量信息\n",
    "    def read_timeseries(self, ts_filename):\n",
    "        ret = []\n",
    "        timeseries = pd.read_csv(ts_filename)\n",
    "        timeseries = timeseries.fillna('')\n",
    "        header = timeseries.columns.values.tolist()\n",
    "        assert header[0] == 'Hours'\n",
    "        for index, row in timeseries.iterrows():\n",
    "            variables = [row[header[i]] for i in range(len(header))]\n",
    "            ret.append(np.array(variables))\n",
    "        return (np.stack(ret), header)\n",
    "    def read_example(self, index):\n",
    "        \"\"\" \n",
    "        Read the example with given index.\n",
    "\n",
    "            :param index: Index of the line of the listfile to read (counting starts from 0).\n",
    "            :return: Directory with the following keys:\n",
    "                X : np.array\n",
    "                    2D array containing all events. Each row corresponds to a moment.\n",
    "                    First column is the time and other columns correspond to different\n",
    "                    variables.\n",
    "                t : float\n",
    "                    Length of the data in hours. Note, in general, it is not equal to the\n",
    "                    timestamp of last event.\n",
    "                y : int (0 or 1)\n",
    "                    Mortality within next 24 hours.\n",
    "                header : array of strings\n",
    "                    Names of the columns. The ordering of the columns is always the same.\n",
    "                name: Name of the sample.\n",
    "        \"\"\"\n",
    "        if index < 0 or index >= len(self.data):\n",
    "            return ValueError(\"Index must be from 0 (inclusive) to number of examples (exclusive).\")\n",
    "        name = self.data_x[index]\n",
    "        name_path = os.path.join(dataset_dir, partition, name)\n",
    "        y = self.data_y[index]\n",
    "        t = self.period_length\n",
    "        (X, header) = self.read_timeseries(name_path)\n",
    "        return {\n",
    "            'X': X,\n",
    "            't': t,\n",
    "            'y': y,\n",
    "            'header': header,\n",
    "            'name': name\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\" #train_reader.read_example(0)\\nprint(train_reader.get_number_of_examples())\\nexample = train_reader.read_example(0)['X']\\nd, h = discretizer.transform(example, end=48.0)\\n \""
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "\"\"\" #train_reader.read_example(0)\n",
    "print(train_reader.get_number_of_examples())\n",
    "example = train_reader.read_example(0)['X']\n",
    "d, h = discretizer.transform(example, end=48.0)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reader = InHospitalMortalityReader(dataset_dir=dataset_dir, partition='train')\n",
    "n_samples = train_reader.get_number_of_examples()\n",
    "# create the discretizer\n",
    "discretizer = Discretizer(timestep=timestep,\n",
    "                            store_masks=store_masks,\n",
    "                            impute_strategy=impute_strategy,\n",
    "                            start_time=start_time)\n",
    "# 找离散变量所在的下标\n",
    "discretizer_header = discretizer.transform(train_reader.read_example(0)[\"X\"])[1].split(',')\n",
    "continuous_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "#print(continuous_channels)\n",
    "normalizer = Normalizer(fields=continuous_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Iterating over train patients' icu stays: 100%|██████████| 5/5 [00:00<00:00, 21.95it/s]\n",
      " Saving the state in result\\ihm_ts_1.00_impute_previous_start_zero_masks_True_n_14803_normalizer.pickle ...\n",
      "saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(n), desc='Iterating over train patients\\' icu stays'):\n",
    "    ret = train_reader.read_example(i)\n",
    "    #print(ret)\n",
    "    data, new_header = discretizer.transform(ret['X'], end=ret['t'])\n",
    "    #print(data[0])\n",
    "    normalizer._feed_data(data)\n",
    "\n",
    "file_name = '{}_ts_{:.2f}_impute_{}_start_{}_masks_{}_n_{}_normalizer.pickle'.format(\n",
    "       task, timestep, impute_strategy, start_time, store_masks, n_samples)\n",
    "file_name = os.path.join(output_dir, file_name)\n",
    "print('\\n Saving the state in {} ...'.format(file_name))\n",
    "normalizer._save_params(file_name)\n",
    "print('saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n76\n60.28333333333333\n0.27108333333333345\n115.4875\n89.50833333333333\n170.0\n77.93055408795675\n98.01666666666667\n21.370833333333334\n117.92916666666666\n36.943203373661746\n77.22818382666668\n7.348416666666668\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "load_file_path = os.path.join(output_dir, 'ihm_ts_1.00_impute_previous_start_zero_masks_True_n_14803_normalizer.pickle')\n",
    "with open(load_file_path, \"rb\") as load_file:\n",
    "    dct = pickle.load(load_file, encoding='latin1')\n",
    "print(len(dct))\n",
    "means = dct['means']\n",
    "stds = dct['stds']\n",
    "print(len(means))\n",
    "for i in continuous_channels :\n",
    "    print(means[i])"
   ]
  }
 ]
}